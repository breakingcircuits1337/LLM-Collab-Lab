// This is an AI-generated file. Do not edit directly.

'use server';

/**
 * @fileOverview Orchestrates sequential calls to various LLMs, passing the output of one to the next.
 *
 * - orchestrateLLMs - A function that orchestrates LLM calls based on a chain length.
 * - LLMOrchestrationInput - The input type for the orchestrateLLMs function.
 * - LLMOrchestrationOutput - The return type for the orchestrateLLMs function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const LLMOrchestrationInputSchema = z.object({
  initialIdea: z.string().describe('The initial idea or prompt from the user.'),
  chainLength: z.number().describe('The number of LLM calls to chain together.'),
});
export type LLMOrchestrationInput = z.infer<typeof LLMOrchestrationInputSchema>;

const LLMOrchestrationOutputSchema = z.object({
  finalIdea: z.string().describe('The final generated idea after multiple LLM calls.'),
});
export type LLMOrchestrationOutput = z.infer<typeof LLMOrchestrationOutputSchema>;

export async function orchestrateLLMs(input: LLMOrchestrationInput): Promise<LLMOrchestrationOutput> {
  return llmOrchestrationFlow(input);
}

const llmOrchestrationPrompt = ai.definePrompt({
  name: 'llmOrchestrationPrompt',
  input: {
    schema: z.object({
      idea: z.string().describe('The current idea to refine.'),
    }),
  },
  output: {
    schema: z.object({
      refinedIdea: z.string().describe('The refined idea generated by the LLM.'),
    }),
  },
  prompt: `Refine the following idea:

  {{{idea}}}
  `,
});

const llmOrchestrationFlow = ai.defineFlow(
  {
    name: 'llmOrchestrationFlow',
    inputSchema: LLMOrchestrationInputSchema,
    outputSchema: LLMOrchestrationOutputSchema,
  },
  async input => {
    let currentIdea = input.initialIdea;

    for (let i = 0; i < input.chainLength; i++) {
      const {output} = await llmOrchestrationPrompt({
        idea: currentIdea,
      });
      currentIdea = output!.refinedIdea;
    }

    return {finalIdea: currentIdea};
  }
);
